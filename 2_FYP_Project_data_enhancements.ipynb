{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfc16d2",
   "metadata": {},
   "source": [
    "## Data enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155a0e9",
   "metadata": {},
   "source": [
    "### Pre-processing images in data\n",
    "** note that only training data will undergo enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea91a27",
   "metadata": {},
   "source": [
    "#### Image enhancement\n",
    "The process of improving the appearance of images to highlight specific features, reduce noise or improve the quality of the image, this helps it to be more suitable for analysis.\n",
    "\n",
    "Alpha focuses on the contrast of image\n",
    "- alpha greater than 1: images brighter, enhanced contrast\n",
    "- alpha less than 1: images darker, reduced contrast\n",
    "\n",
    "Beta focuses on brightness of image\n",
    "- beta positive: makes images brighter\n",
    "- beta negative: makes images darker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becbd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/csv/train_data.csv\")\n",
    "test_data = pd.read_csv(\"datasets/csv/test_data.csv\")\n",
    "\n",
    "## PREPROCESSING IMAGES WITH ENHANCEMENT\n",
    "def preprocess_image(img_path, enhance):\n",
    "    # reading images\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if enhance == True:\n",
    "        # enhancing image\n",
    "        img = cv2.convertScaleAbs(img, alpha = 1.5, beta = -20)\n",
    "    # target_size of 224, 224 commonly used for image classification\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    # normalising pixel values\n",
    "    img_array = img.astype(np.float32) / 255\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4661e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using preprocessed images as train data\n",
    "train_images = np.array([preprocess_image(image_path, True) for image_path in train_data[\"image_path\"]])\n",
    "# using \"pathology\" column as train labels\n",
    "train_labels = np.array(train_data[\"pathology\"])\n",
    "\n",
    "# change \"BENIGN_WITHOUT_CALLBACK\" to \"BENIGN\"\n",
    "train_labels[train_labels == \"BENIGN_WITHOUT_CALLBACK\"] = \"BENIGN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac431677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using preprocessed images as test data\n",
    "test_images = np.array([preprocess_image(image_path, False) for image_path in test_data[\"image_path\"]])\n",
    "# using \"pathology\" column as test labels\n",
    "test_labels = np.array(test_data[\"pathology\"])\n",
    "\n",
    "# change \"BENIGN_WITHOUT_CALLBACK\" to \"BENIGN\"\n",
    "test_labels[test_labels == \"BENIGN_WITHOUT_CALLBACK\"] = \"BENIGN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e925d57",
   "metadata": {},
   "source": [
    "#### Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c318cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMAGE AUGMENTATION - using numpy\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "# augmenting images and storing in lists\n",
    "for i, img_path in enumerate(train_data[\"image_path\"]):\n",
    "    original_image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    enhanced_image = preprocess_image(img_path, True)\n",
    "    \n",
    "    # adding original image and label\n",
    "    augmented_images.append(enhanced_image)\n",
    "    augmented_labels.append(train_labels[i])\n",
    "    \n",
    "    # making all combinations of flips\n",
    "    for horizontal_flip in [True, False]:\n",
    "        for vertical_flip in [True, False]:\n",
    "            # applying flips\n",
    "            augmented_image = enhanced_image\n",
    "            if horizontal_flip:\n",
    "                augmented_image = np.fliplr(augmented_image)\n",
    "            if vertical_flip:\n",
    "                augmented_image = np.flipud(augmented_image)\n",
    "                \n",
    "            # adding augmented image and label\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_labels.append(train_labels[i])\n",
    "        \n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1605c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_aug_train_labels = label_encoder.fit_transform(augmented_labels)\n",
    "encoded_test_labels = label_encoder.fit_transform(test_labels)\n",
    "# one-hot encode labels\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(encoded_train_labels)\n",
    "one_hot_aug_train_labels = tf.keras.utils.to_categorical(encoded_aug_train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7228841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## VERSION 2 OF IMAGE AUGMENTATION - using TensorFlow\n",
    "## datagen = ImageDataGenerator(\n",
    "##     horizontal_flip = True,\n",
    "##     vertical_flip = True,\n",
    "##     fill_mode = \"nearest\"\n",
    "## )\n",
    "## \n",
    "## augmented_generator = datagen.flow(train_images[:, :, :, np.newaxis], one_hot_train_labels, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5ffaa",
   "metadata": {},
   "source": [
    "## Creating of models\n",
    "- **Conv2D** performs convolutional operations on the input image data. It applies a set of filters to the input images to extract features.\n",
    "- **MaxPooling2D** is a down-sampling operation that reduces the spatial dimensions, used after Conv2D layers to retain the most important information.\n",
    "- **Flatten** is used to convert the multi-dimensional output of the previous laters into 1D.\n",
    "- **Dense** represents a fully connected layer, where each neuron or node is connected to every neuron in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9248ca",
   "metadata": {},
   "source": [
    "### Base model\n",
    "Ensures that the data can be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bebd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Sequential()\n",
    "# creating stack of Conv2D and MaxPooling2D\n",
    "base_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "base_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(128, activation = \"relu\"))\n",
    "# output layer with softmax\n",
    "base_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce4ade",
   "metadata": {},
   "source": [
    "It was noted that whilst the adam optimizer had the highest accuracy, the nadam optimizer has a higher validation accuracy, which would suggest a change in choice to the preliminary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12218cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "368/368 [==============================] - 282s 760ms/step - loss: 0.8070 - accuracy: 0.6009 - val_loss: 1.1220 - val_accuracy: 0.5391\n",
      "Epoch 2/10\n",
      "368/368 [==============================] - 276s 750ms/step - loss: 0.5557 - accuracy: 0.7079 - val_loss: 1.1438 - val_accuracy: 0.5119\n",
      "Epoch 3/10\n",
      "368/368 [==============================] - 277s 753ms/step - loss: 0.4355 - accuracy: 0.7987 - val_loss: 3.4655 - val_accuracy: 0.5527\n",
      "Epoch 4/10\n",
      "368/368 [==============================] - 277s 752ms/step - loss: 0.2964 - accuracy: 0.8842 - val_loss: 2.0180 - val_accuracy: 0.5255\n",
      "Epoch 5/10\n",
      "368/368 [==============================] - 278s 755ms/step - loss: 0.1775 - accuracy: 0.9424 - val_loss: 2.5056 - val_accuracy: 0.5612\n",
      "Epoch 6/10\n",
      "368/368 [==============================] - 277s 753ms/step - loss: 0.1082 - accuracy: 0.9714 - val_loss: 2.5110 - val_accuracy: 0.5595\n",
      "Epoch 7/10\n",
      "368/368 [==============================] - 278s 755ms/step - loss: 0.0760 - accuracy: 0.9852 - val_loss: 4.2846 - val_accuracy: 0.5595\n",
      "Epoch 8/10\n",
      "368/368 [==============================] - 279s 759ms/step - loss: 0.0602 - accuracy: 0.9892 - val_loss: 2.6264 - val_accuracy: 0.5272\n",
      "Epoch 9/10\n",
      "368/368 [==============================] - 281s 764ms/step - loss: 0.0521 - accuracy: 0.9921 - val_loss: 3.1003 - val_accuracy: 0.5340\n",
      "Epoch 10/10\n",
      "368/368 [==============================] - 222s 604ms/step - loss: 0.0479 - accuracy: 0.9922 - val_loss: 4.8636 - val_accuracy: 0.5510\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "base_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = base_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a5680",
   "metadata": {},
   "source": [
    "### Improving with additional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40eafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoLayer_model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "twoLayer_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "twoLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "twoLayer_model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "twoLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "twoLayer_model.add(Flatten())\n",
    "twoLayer_model.add(Dense(128, activation = \"relu\"))\n",
    "# using dropout for regularisation (reduces overfitting)\n",
    "twoLayer_model.add(Dropout(0.5))\n",
    "# output layer with softmax\n",
    "twoLayer_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc11b8be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "368/368 [==============================] - 301s 815ms/step - loss: 0.6831 - accuracy: 0.5898 - val_loss: 0.9790 - val_accuracy: 0.5391\n",
      "Epoch 2/10\n",
      "368/368 [==============================] - 303s 823ms/step - loss: 0.5776 - accuracy: 0.6810 - val_loss: 2.0178 - val_accuracy: 0.5170\n",
      "Epoch 3/10\n",
      "368/368 [==============================] - 302s 820ms/step - loss: 0.4720 - accuracy: 0.7631 - val_loss: 2.9918 - val_accuracy: 0.5493\n",
      "Epoch 4/10\n",
      "368/368 [==============================] - 303s 824ms/step - loss: 0.3502 - accuracy: 0.8405 - val_loss: 3.0136 - val_accuracy: 0.5578\n",
      "Epoch 5/10\n",
      "368/368 [==============================] - 410s 1s/step - loss: 0.2529 - accuracy: 0.8965 - val_loss: 4.1739 - val_accuracy: 0.5391\n",
      "Epoch 6/10\n",
      "368/368 [==============================] - 419s 1s/step - loss: 0.1959 - accuracy: 0.9236 - val_loss: 7.5749 - val_accuracy: 0.5527\n",
      "Epoch 7/10\n",
      "368/368 [==============================] - 417s 1s/step - loss: 0.1680 - accuracy: 0.9401 - val_loss: 5.6220 - val_accuracy: 0.5391\n",
      "Epoch 8/10\n",
      "368/368 [==============================] - 418s 1s/step - loss: 0.1225 - accuracy: 0.9547 - val_loss: 6.4133 - val_accuracy: 0.5136\n",
      "Epoch 9/10\n",
      "368/368 [==============================] - 419s 1s/step - loss: 0.1205 - accuracy: 0.9609 - val_loss: 5.5139 - val_accuracy: 0.5510\n",
      "Epoch 10/10\n",
      "368/368 [==============================] - 420s 1s/step - loss: 0.1041 - accuracy: 0.9682 - val_loss: 6.3527 - val_accuracy: 0.5391\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "twoLayer_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = twoLayer_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0271c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeLayer_model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "threeLayer_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "threeLayer_model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# third convolutional layer\n",
    "threeLayer_model.add(Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "threeLayer_model.add(Flatten())\n",
    "threeLayer_model.add(Dense(128, activation = \"relu\"))\n",
    "# using dropout for regularisation (reduces overfitting)\n",
    "threeLayer_model.add(Dropout(0.5))\n",
    "# output layer with softmax\n",
    "threeLayer_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "468faff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "368/368 [==============================] - 532s 1s/step - loss: 0.6680 - accuracy: 0.5797 - val_loss: 1.4812 - val_accuracy: 0.5476\n",
      "Epoch 2/10\n",
      "368/368 [==============================] - 532s 1s/step - loss: 0.6202 - accuracy: 0.6213 - val_loss: 1.1769 - val_accuracy: 0.5578\n",
      "Epoch 3/10\n",
      "368/368 [==============================] - 531s 1s/step - loss: 0.5636 - accuracy: 0.6840 - val_loss: 1.1940 - val_accuracy: 0.5119\n",
      "Epoch 4/10\n",
      "368/368 [==============================] - 555s 2s/step - loss: 0.4830 - accuracy: 0.7457 - val_loss: 3.0069 - val_accuracy: 0.5578\n",
      "Epoch 5/10\n",
      "368/368 [==============================] - 462s 1s/step - loss: 0.3986 - accuracy: 0.8018 - val_loss: 2.0103 - val_accuracy: 0.5578\n",
      "Epoch 6/10\n",
      "368/368 [==============================] - 386s 1s/step - loss: 0.3171 - accuracy: 0.8513 - val_loss: 2.2953 - val_accuracy: 0.4983\n",
      "Epoch 7/10\n",
      "368/368 [==============================] - 2796s 8s/step - loss: 0.2646 - accuracy: 0.8800 - val_loss: 4.1996 - val_accuracy: 0.5578\n",
      "Epoch 8/10\n",
      "368/368 [==============================] - 531s 1s/step - loss: 0.2101 - accuracy: 0.9066 - val_loss: 3.4005 - val_accuracy: 0.5612\n",
      "Epoch 9/10\n",
      "368/368 [==============================] - 534s 1s/step - loss: 0.1935 - accuracy: 0.9185 - val_loss: 2.2042 - val_accuracy: 0.5748\n",
      "Epoch 10/10\n",
      "368/368 [==============================] - 530s 1s/step - loss: 0.1607 - accuracy: 0.9341 - val_loss: 3.7118 - val_accuracy: 0.5731\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "threeLayer_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = threeLayer_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
