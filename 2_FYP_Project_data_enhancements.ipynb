{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfc16d2",
   "metadata": {},
   "source": [
    "## Data enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a230bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155a0e9",
   "metadata": {},
   "source": [
    "### Pre-processing images in data\n",
    "** note that only training data will undergo enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea91a27",
   "metadata": {},
   "source": [
    "#### Image enhancement\n",
    "The process of improving the appearance of images to highlight specific features, reduce noise or improve the quality of the image, this helps it to be more suitable for analysis.\n",
    "\n",
    "Alpha focuses on the contrast of image\n",
    "- alpha greater than 1: images brighter, enhanced contrast\n",
    "- alpha less than 1: images darker, reduced contrast\n",
    "\n",
    "Beta focuses on brightness of image\n",
    "- beta positive: makes images brighter\n",
    "- beta negative: makes images darker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becbd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/csv/train_data.csv\")\n",
    "test_data = pd.read_csv(\"datasets/csv/test_data.csv\")\n",
    "\n",
    "## PREPROCESSING IMAGES WITH ENHANCEMENT\n",
    "def preprocess_image(img_path, enhance):\n",
    "    # reading images\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if enhance == True:\n",
    "        # enhancing image\n",
    "        img = cv2.convertScaleAbs(img, alpha = 1.5, beta = -20)\n",
    "    # target_size of 224, 224 commonly used for image classification\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    # normalising pixel values\n",
    "    img_array = img.astype(np.float32) / 255\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4661e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using preprocessed images as train data\n",
    "train_images = np.array([preprocess_image(image_path, True) for image_path in train_data[\"image_path\"]])\n",
    "# using \"pathology\" column as train labels\n",
    "train_labels = np.array(train_data[\"pathology\"])\n",
    "\n",
    "# change \"BENIGN_WITHOUT_CALLBACK\" to \"BENIGN\"\n",
    "train_labels[train_labels == \"BENIGN_WITHOUT_CALLBACK\"] = \"BENIGN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac431677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using preprocessed images as test data\n",
    "test_images = np.array([preprocess_image(image_path, False) for image_path in test_data[\"image_path\"]])\n",
    "# using \"pathology\" column as test labels\n",
    "test_labels = np.array(test_data[\"pathology\"])\n",
    "\n",
    "# change \"BENIGN_WITHOUT_CALLBACK\" to \"BENIGN\"\n",
    "test_labels[test_labels == \"BENIGN_WITHOUT_CALLBACK\"] = \"BENIGN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e925d57",
   "metadata": {},
   "source": [
    "#### Image augmentation\n",
    "The augmented image stores the following:\n",
    "- original image without enhancement\n",
    "- enhanced image with enhancement\n",
    "- all combinations of augmented flips (with enhancement)\n",
    "- all combinations of augmented flips (without enhancement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c318cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18816\n",
      "18816\n"
     ]
    }
   ],
   "source": [
    "## IMAGE AUGMENTATION - using numpy\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "# augmenting images and storing in lists\n",
    "for i, img_path in enumerate(train_data[\"image_path\"]):\n",
    "    original_image = preprocess_image(img_path, False)\n",
    "    enhanced_image = preprocess_image(img_path, True)\n",
    "    \n",
    "    # making all combinations of flips\n",
    "    for horizontal_flip in [True, False]:\n",
    "        for vertical_flip in [True, False]:\n",
    "            # applying flips on original image\n",
    "            augmented_image = original_image\n",
    "            if horizontal_flip:\n",
    "                augmented_image = np.fliplr(augmented_image)\n",
    "            if vertical_flip:\n",
    "                augmented_image = np.flipud(augmented_image)\n",
    "                \n",
    "            # adding augmented image and label\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_labels.append(train_labels[i])\n",
    "            \n",
    "            # applying flips on enhanced image\n",
    "            augmented_image = enhanced_image\n",
    "            if horizontal_flip:\n",
    "                augmented_image = np.fliplr(augmented_image)\n",
    "            if vertical_flip:\n",
    "                augmented_image = np.flipud(augmented_image)\n",
    "                \n",
    "            # adding augmented image and label\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_labels.append(train_labels[i])\n",
    "        \n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "print(len(train_labels) * 8)\n",
    "print(len(augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1605c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "encoded_aug_train_labels = label_encoder.fit_transform(augmented_labels)\n",
    "encoded_test_labels = label_encoder.fit_transform(test_labels)\n",
    "# one-hot encode labels\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(encoded_train_labels)\n",
    "one_hot_aug_train_labels = tf.keras.utils.to_categorical(encoded_aug_train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7228841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## VERSION 2 OF IMAGE AUGMENTATION - using TensorFlow\n",
    "## datagen = ImageDataGenerator(\n",
    "##     horizontal_flip = True,\n",
    "##     vertical_flip = True,\n",
    "##     fill_mode = \"nearest\"\n",
    "## )\n",
    "## \n",
    "## augmented_generator = datagen.flow(train_images[:, :, :, np.newaxis], one_hot_train_labels, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5ffaa",
   "metadata": {},
   "source": [
    "## Creating of models\n",
    "- **Conv2D** performs convolutional operations on the input image data. It applies a set of filters to the input images to extract features.\n",
    "- **MaxPooling2D** is a down-sampling operation that reduces the spatial dimensions, used after Conv2D layers to retain the most important information.\n",
    "- **Flatten** is used to convert the multi-dimensional output of the previous laters into 1D.\n",
    "- **Dense** represents a fully connected layer, where each neuron or node is connected to every neuron in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1cb64",
   "metadata": {},
   "source": [
    "### Base model\n",
    "Ensures that the data can be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bebd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Sequential()\n",
    "# creating stack of Conv2D and MaxPooling2D\n",
    "base_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "base_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Dense(128, activation = \"relu\"))\n",
    "# output layer with softmax\n",
    "base_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce4ade",
   "metadata": {},
   "source": [
    "It was noted that whilst the adam optimizer had the highest accuracy, the nadam optimizer has a higher validation accuracy, which would suggest a change in choice to the preliminary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12218cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "588/588 [==============================] - 340s 574ms/step - loss: 0.7152 - accuracy: 0.5622 - val_loss: 0.6501 - val_accuracy: 0.5425\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 335s 570ms/step - loss: 0.6356 - accuracy: 0.6129 - val_loss: 0.6775 - val_accuracy: 0.6020\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 335s 570ms/step - loss: 0.6042 - accuracy: 0.6399 - val_loss: 0.6590 - val_accuracy: 0.6156\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 339s 576ms/step - loss: 0.5551 - accuracy: 0.6814 - val_loss: 0.6525 - val_accuracy: 0.6224\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 339s 576ms/step - loss: 0.5064 - accuracy: 0.7197 - val_loss: 0.7573 - val_accuracy: 0.6003\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 338s 574ms/step - loss: 0.4726 - accuracy: 0.7453 - val_loss: 0.7870 - val_accuracy: 0.6105\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 337s 574ms/step - loss: 0.4469 - accuracy: 0.7661 - val_loss: 0.7687 - val_accuracy: 0.6139\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 338s 575ms/step - loss: 0.3989 - accuracy: 0.7785 - val_loss: 0.9420 - val_accuracy: 0.5969\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 337s 574ms/step - loss: 0.3356 - accuracy: 0.7968 - val_loss: 1.0913 - val_accuracy: 0.6327\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 338s 574ms/step - loss: 0.2870 - accuracy: 0.8472 - val_loss: 1.1120 - val_accuracy: 0.6276\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "base_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = base_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c9383",
   "metadata": {},
   "source": [
    "### Improving with additional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb01203",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoLayer_model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "twoLayer_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "twoLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "twoLayer_model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "twoLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "twoLayer_model.add(Flatten())\n",
    "twoLayer_model.add(Dense(128, activation = \"relu\"))\n",
    "# using dropout for regularisation (reduces overfitting)\n",
    "twoLayer_model.add(Dropout(0.5))\n",
    "# output layer with softmax\n",
    "twoLayer_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8581077",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "588/588 [==============================] - 560s 949ms/step - loss: 0.6667 - accuracy: 0.5936 - val_loss: 0.6408 - val_accuracy: 0.6020\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 668s 1s/step - loss: 0.5646 - accuracy: 0.6898 - val_loss: 0.6442 - val_accuracy: 0.6497\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 669s 1s/step - loss: 0.4201 - accuracy: 0.7912 - val_loss: 0.7147 - val_accuracy: 0.6463\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 670s 1s/step - loss: 0.2899 - accuracy: 0.8685 - val_loss: 0.8466 - val_accuracy: 0.6514\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 670s 1s/step - loss: 0.2023 - accuracy: 0.9182 - val_loss: 0.9618 - val_accuracy: 0.6412\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 672s 1s/step - loss: 0.1505 - accuracy: 0.9455 - val_loss: 1.1651 - val_accuracy: 0.6173\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 672s 1s/step - loss: 0.1258 - accuracy: 0.9538 - val_loss: 1.2172 - val_accuracy: 0.6344\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 672s 1s/step - loss: 0.0936 - accuracy: 0.9658 - val_loss: 1.4868 - val_accuracy: 0.6310\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 670s 1s/step - loss: 0.0907 - accuracy: 0.9692 - val_loss: 1.3662 - val_accuracy: 0.6480\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 666s 1s/step - loss: 0.0825 - accuracy: 0.9730 - val_loss: 1.5266 - val_accuracy: 0.6531\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "twoLayer_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = twoLayer_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fae5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeLayer_model = Sequential()\n",
    "\n",
    "# first convolutional layer\n",
    "threeLayer_model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (224, 224, 1)))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# second convolutional layer\n",
    "threeLayer_model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# third convolutional layer\n",
    "threeLayer_model.add(Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "threeLayer_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# unrolling output to 1D\n",
    "threeLayer_model.add(Flatten())\n",
    "threeLayer_model.add(Dense(128, activation = \"relu\"))\n",
    "# using dropout for regularisation (reduces overfitting)\n",
    "threeLayer_model.add(Dropout(0.5))\n",
    "# output layer with softmax\n",
    "threeLayer_model.add(Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa3c139",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "588/588 [==============================] - 840s 1s/step - loss: 0.6575 - accuracy: 0.5903 - val_loss: 0.6392 - val_accuracy: 0.6037\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 837s 1s/step - loss: 0.5863 - accuracy: 0.6653 - val_loss: 0.6382 - val_accuracy: 0.5952\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 844s 1s/step - loss: 0.4748 - accuracy: 0.7582 - val_loss: 0.7374 - val_accuracy: 0.6276\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 29545s 50s/step - loss: 0.3396 - accuracy: 0.8432 - val_loss: 0.8523 - val_accuracy: 0.6497\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 587s 998ms/step - loss: 0.2328 - accuracy: 0.9051 - val_loss: 1.0398 - val_accuracy: 0.6224\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 600s 1s/step - loss: 0.1755 - accuracy: 0.9347 - val_loss: 1.1613 - val_accuracy: 0.6139\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 591s 1s/step - loss: 0.1277 - accuracy: 0.9562 - val_loss: 1.2701 - val_accuracy: 0.6241\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 590s 1s/step - loss: 0.1083 - accuracy: 0.9636 - val_loss: 1.5463 - val_accuracy: 0.6071\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 583s 991ms/step - loss: 0.0903 - accuracy: 0.9683 - val_loss: 1.6469 - val_accuracy: 0.6122\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 589s 1s/step - loss: 0.0813 - accuracy: 0.9734 - val_loss: 1.6336 - val_accuracy: 0.6139\n"
     ]
    }
   ],
   "source": [
    "# compile model, improving accuracy\n",
    "threeLayer_model.compile(optimizer = \"Nadam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "# train model, validating on test set\n",
    "history = threeLayer_model.fit(augmented_images, one_hot_aug_train_labels, epochs = 10, validation_data = (test_images, one_hot_test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
